---
title: "Neural Network Regression"
author: "Karen Mazidi"
output:  
  pdf_document: default
  html_document: default
editor_options: 
  chunk_output_type: inline
---

In this notebook we try a neural network on the Boston housing data. First we perform linear regression on the data.

### Linear regression

The correlation is 0.888 and the mse is 23.094.

```{r}
library(MASS)
data("Boston")
df_boston <- Boston[]
set.seed(1234)
i <- sample(1:nrow(Boston), 0.75*nrow(Boston), replace=FALSE)
train <- df_boston[i,]
test <- df_boston[-i,]
lm1 <- lm(medv~., data=train)
summary(lm1)
pred1 <- predict(lm1, newdata=test)
cor1 <- cor(pred1, test$medv)
mse1 <- mean((pred1-test$medv)^2)
```

### Data preprocessing

The neural network will perform better if the data is scaled. However, to analyze our results we will need to unscale it to get the original data back. We could use the unscale() function in package DMwR as demonstrated in Section 8.4, but instead we are going to scale manually. We write a function to normalize the data.


```{r}
# function to normalize the data
normalize <- function(x){
  return ((x - min(x)) / (max(x) - min(x)))
}

# apply to all columns
train_scaled <- as.data.frame(lapply(train, normalize))
test_scaled <- as.data.frame(lapply(test, normalize))

```

### neural network


Unfortunately, neuralnet() can't handle regular R formulas so we build a string to hold the formula.

For the neuralnet() function we input the formula, the data and specify we want two hidden layers, the first with 6 hidden nodes and the second with 3. The linear.output parameter should be TRUE for regression and FALSE for classification.

How many nodes should we have in hidden layers? If there are too many it might cause overfitting but too few can cause underfitting. Different rules of thumb that are commonly discussed are:

* between 1 and the number of predictors
* 2/3 of the input layer size plus the size of the output layer
* less than twice the input layer size

These rules of thumbs suggest 1-13, 9, and < 26. Let's try 9 hidden nodes, 6 and 3. 

The neural net algorithm initializes weights randomly (unless we specify initial weights). This random start will produce slightly different results each run. For this reason we set a seed to get reproducible results. 

```{r}
library(neuralnet)
n <- names(train)
f <- as.formula(paste("medv ~", paste(n[!n %in% "medv"], collapse = " + ")))
set.seed(1234)
nn1 <- neuralnet(f, data=train_scaled, hidden=c(6, 3),
                 linear.output = TRUE)
plot(nn1)
```

### results on test data

Correlation: 0.951
MSE: 15.479

This is an improvement over the linear regression model. 

```{r}
pred2 <- compute(nn1, test_scaled[,1:13])
# scaled correlation
cor2_scaled <- cor(pred2$net.result, test_scaled$medv)
# unscaled original values correlation
pred2_unscaled <- pred2$net.result * (max(test$medv) - min(test$medv)) + min(test$medv)
cor2_unscaled <- cor(pred2_unscaled, test_scaled$medv)
mse2 <- mean((pred2_unscaled - test$medv)^2)
```

