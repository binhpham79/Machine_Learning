---
title: "Beijing PM2.5 Data"
output:
  pdf_document: default
  html_notebook: default
  html_document:
    df_print: paged
---

Hourly data of PM2.5 from the US Embassy in Beijing. PM2.5 is a measure of particulate matter that have a diameter of less than 2.5 micrometers. They are an important measure of air quality for humans. 

The data was downloaded from the UCI Machine Learning Repository and is calleed the [Beijing PM2.5 Data Set](https://archive.ics.uci.edu/ml/datasets/Beijing+PM2.5+Data#)

### Load data

Loading the data and restricting to complete cases leaves about 41K observations. We will remove the No, day, and hour columns. Remaining columns are the year and month, pm2.5, temperature, pressure, combined wind direction. cumulated wind speed, cumulated hours of snow and cumulated hours of rain.

```{r}
df <- read.csv("PRSA_data.csv", header=TRUE)
df <- df[complete.cases(df), c(3, 6:13)]
head(df)
str(df)
```


### Train/test split

```{r}
set.seed(1234)
i <- sample(1:nrow(df), 0.8*nrow(df), replace=FALSE)
train <- df[i,]
test <- df[-i,]
```

### Try linear regression

Correlation is about 0.5, not bad but not impressive either. The mse is 6232. The rmse is about 79. Since the range of pm2.5 in the test data is 0:886 this is about 10%. Not really that bad. 

```{r}
# build a model
lm1 <- lm(pm2.5~., data=train)
summary(lm1)
# evaluate
pred <- predict(lm1, newdata=test)
cor_lm <- cor(pred, test$pm2.5)
mse_lm <- mean((pred-test$pm2.5)^2)
```


### normalize data

```{r}
train$cbwd <- as.integer(train$cbwd)
test$cbwd <- as.integer(test$cbwd)
normalize <- function(x){
  return ((x - min(x)) / (max(x) - min(x)))
}
train_scaled <- as.data.frame(lapply(train, normalize))
test_scaled <- as.data.frame(lapply(test, normalize))
```


### Try knn

Correlation is up to 0.7. MSE is down to 4302. Much better, even without scaling and just randomly picking a k.

```{r}
library(caret)
fit <- knnreg(train_scaled[,-2], train_scaled[,2], k=7)
# evaluate
pred <- predict(fit, test_scaled[,-2])
pred <- pred * (max(test$pm2.5) - min(test$pm2.5)) + min(test$pm2.5)
cor_knn <- cor(pred, test$pm2.5)
mse_knn <- mean((pred-test$pm2.5)^2)
```



### Try neural network

Ran in to a series of problems:
- turned on verbose output with lifesign="full" and realized threshold too high
- raised threshold to 0.04, only 10K train; performance between linear and knn
- the following is a run on a smaller training set to test the architecture
 

```{r}
library(neuralnet)
n <- names(train_scaled)
f <- as.formula(paste("pm2.5 ~", paste(n[!n %in% "pm2.5"], collapse = " + ")))
set.seed(1234)
j <- sample(1:nrow(train), 10000, replace=FALSE)
train_small <- train_scaled[j,]
set.seed(1234)  # this should not be necessary but it is
nn1 <- neuralnet(f, data=train_small, hidden=c(6,3), threshold = 0.05, linear.output=TRUE)
plot(nn1)
# evaluate
pred <- compute(nn1, test_scaled[,-2])
pred <- pred$net.result
pred_unscale <- pred * (max(test$pm2.5) - min(test$pm2.5)) + min(test$pm2.5)
cor_nn1 <- cor(pred_unscale, test$pm2.5)
mse_nn1 <- mean((pred_unscale - test$pm2.5)^2)
nn1$result.matrix

# look at weights
par(mfrow=c(2,2))
gwplot(nn1, selected.covariate = "month", min=-2.5, max=5)
gwplot(nn1, selected.covariate = "DEWP", min=-2.5, max=5)
gwplot(nn1, selected.covariate = "TEMP", min=-2.5, max=5)
gwplot(nn1, selected.covariate = "PRES", min=-2.5, max=5)

```

For month, the weights are negative for the first few months and positive for the latter ones. DEWP weights are positive while TEMP and PRES weights are negative. If you see a predictor for which the weights are consistently 0, that tells you that the predictor is not useful.



### Try on full data

```{r}
set.seed(1234)  # this should not be necessary but it is
nn2 <- neuralnet(f, data=train_scaled, hidden=c(6,3), threshold = 0.05,  linear.output=TRUE)
# evaluate
pred <- compute(nn2, test_scaled[,-2])
pred <- pred$net.result
pred_unscale <- pred * (max(test$pm2.5) - min(test$pm2.5)) + min(test$pm2.5)
cor_nn2 <- cor(pred_unscale, test$pm2.5)
mse_nn2 <- mean((pred_unscale - test$pm2.5)^2)
nn2$result.matrix
```

### Results so far

```{r}
print(paste("cor and mse for linear regression: ", cor_lm, mse_lm))
print(paste("cor and mse for nn on 10K training data: ", cor_nn1, mse_nn1))
print(paste("cor and mse for nn on 33K training data: ", cor_nn2, mse_nn2))

```

Takeaways:
- comparing linear, knn, and nn: knn is the clear favorite
- the fact that linear did not do well and knn did may have been an indication that there is not a linear relationship between the predictors and the response, perhaps we need a more complex architecture


### try a more complex model on all the training data

```{r}
set.seed(1234)  # this should not be necessary but it is
nn3 <- neuralnet(f, data=train_scaled, hidden=c(6,4,2), threshold = 0.12, linear.output=TRUE)
# evaluate
pred <- compute(nn3, test_scaled[,-2])
pred <- pred$net.result
pred_unscale <- pred * (max(test$pm2.5) - min(test$pm2.5)) + min(test$pm2.5)
cor_nn3 <- cor(pred_unscale, test$pm2.5)
mse_nn3 <- mean((pred_unscale - test$pm2.5)^2)
# print results
print(paste("cor and mse for nn with 3 hidden layers on 33K training data: ", cor_nn3, mse_nn3))
nn3$result.matrix

```

### other implementations

This took about an hour to run and did not do as well as the neuralnet() algorithm.

Commenting it out.

#```{r}
library(caret)
grid1 <- expand.grid(.decay=c(0.5, 0.1), .size=c(4,5,6))
nnetfit <- train(pm2.5 ~ ., data=train_scaled, method="nnet", maxit=1000, tuneGrid=grid1)
# nnetfit$bestTune size 6 decay 0.1
pred <- predict(nnetfit, newdata=test_scaled)
pred_unscale <- pred * (max(test$pm2.5) - min(test$pm2.5)) + min(test$pm2.5)
cor_nn4 <- cor(pred_unscale, test$pm2.5)
mse_nn4 <- mean((pred_unscale - test$pm2.5)^2)
# cor 0.669  mse 5075
#```


