class_column <- train_scaled$diabetes
n <- length(class_column)
x <- matrix(0, n, length(levels(class_column)))
x[(1L:n) + n * (unclass(class_column) - 1L)] <- 1
dimnames(x) <- list(names(class_column), levels(class_column))
train_scaled <- cbind(train_scaled[,1:8], x)
names(train_scaled) <- c(names(train_scaled)[1:8], "l1", "l2")
n <- names(train)
f <- as.formula(paste("l1 + l2 ~", paste(n[!n %in% "diabetes"], collapse = " + ")))
library(neuralnet)
set.seed(1234)
nnet1 <- neuralnet(f, data=train_scaled, hidden=c(5,2), threshold=0.1, act.fct = "logistic", linear.output=FALSE, lifesign = "minimal")
plot(nnet1)
nnet.results <- compute(nnet1, test_scaled[,1:8])
pred <- max.col(nnet.results$net.result)
mean(test_scaled$diabetes==pred)
scaled <- scale(df[,-9])
library(mlbench)
data("PimaIndiansDiabetes2")
df <- PimaIndiansDiabetes2[complete.cases(PimaIndiansDiabetes2[]),]
set.seed(1234)
i <- sample(1:nrow(df), 0.75*nrow(df), replace=FALSE)
train <- df[i,]
test <- df[-i,]
glm1 <- glm(diabetes~., data=train, family=binomial)
probs <- predict(glm1, newdata=test, type="response")
pred1 <- ifelse(probs>0.5, "pos", "neg")
mean(pred1==test$diabetes)
scaled <- scale(df[,-9])
df_scaled <- data.frame(cbind(scaled, df$diabetes))
colnames(df_scaled) <- colnames(df)
df_scaled$diabetes <- factor(df_scaled$diabetes)
train_scaled <- df_scaled[i,]
test_scaled <- df_scaled[-i,]
class_column <- train_scaled$diabetes
n <- length(class_column)
x <- matrix(0, n, length(levels(class_column)))
x[(1L:n) + n * (unclass(class_column) - 1L)] <- 1
dimnames(x) <- list(names(class_column), levels(class_column))
train_scaled <- cbind(train_scaled[,1:8], x)
names(train_scaled) <- c(names(train_scaled)[1:8], "l1", "l2")
n <- names(train)
f <- as.formula(paste("l1 + l2 ~", paste(n[!n %in% "diabetes"], collapse = " + ")))
library(neuralnet)
set.seed(1234)
nnet1 <- neuralnet(f, data=train_scaled, hidden=c(5,2), threshold=0.1, act.fct = "logistic", linear.output=FALSE, lifesign = "minimal")
plot(nnet1)
nnet.results <- compute(nnet1, test_scaled[,1:8])
pred <- max.col(nnet.results$net.result)
mean(test_scaled$diabetes==pred)
library(mlbench)
data("PimaIndiansDiabetes2")
df <- PimaIndiansDiabetes2[complete.cases(PimaIndiansDiabetes2[]),]
set.seed(1234)
i <- sample(1:nrow(df), 0.75*nrow(df), replace=FALSE)
train <- df[i,]
test <- df[-i,]
glm1 <- glm(diabetes~., data=train, family=binomial)
probs <- predict(glm1, newdata=test, type="response")
pred1 <- ifelse(probs>0.5, "pos", "neg")
mean(pred1==test$diabetes)
scaled <- scale(df[,-9])
df_scaled <- data.frame(cbind(scaled, df$diabetes))
colnames(df_scaled) <- colnames(df)
df_scaled$diabetes <- factor(df_scaled$diabetes)
train_scaled <- df_scaled[i,]
test_scaled <- df_scaled[-i,]
class_column <- train_scaled$diabetes
n <- length(class_column)
x <- matrix(0, n, length(levels(class_column)))
x[(1L:n) + n * (unclass(class_column) - 1L)] <- 1
dimnames(x) <- list(names(class_column), levels(class_column))
train_scaled <- cbind(train_scaled[,1:8], x)
names(train_scaled) <- c(names(train_scaled)[1:8], "pos", "neg")
n <- names(train)
f <- as.formula(paste("pos + neg ~", paste(n[!n %in% "diabetes"], collapse = " + ")))
library(neuralnet)
set.seed(1234)
nnet1 <- neuralnet(f, data=train_scaled, hidden=c(5,2), threshold=0.1, act.fct = "logistic", linear.output=FALSE, lifesign = "minimal")
plot(nnet1)
nnet.results <- compute(nnet1, test_scaled[,1:8])
pred <- max.col(nnet.results$net.result)
mean(test_scaled$diabetes==pred)
head(train_scaled)
head(train)
class_column <- train_scaled$diabetes
n <- length(class_column)
x <- matrix(0, n, length(levels(class_column)))
x[(1L:n) + n * (unclass(class_column) - 1L)] <- 1
dimnames(x) <- list(names(class_column), levels(class_column))
train_scaled <- cbind(train_scaled[,1:8], x)
library(mlbench)
data("PimaIndiansDiabetes2")
df <- PimaIndiansDiabetes2[complete.cases(PimaIndiansDiabetes2[]),]
set.seed(1234)
i <- sample(1:nrow(df), 0.75*nrow(df), replace=FALSE)
train <- df[i,]
test <- df[-i,]
scaled <- scale(df[,-9])
df_scaled <- data.frame(cbind(scaled, df$diabetes))
colnames(df_scaled) <- colnames(df)
df_scaled$diabetes <- factor(df_scaled$diabetes)
train_scaled <- df_scaled[i,]
test_scaled <- df_scaled[-i,]
class_column <- train_scaled$diabetes
n <- length(class_column)
x <- matrix(0, n, length(levels(class_column)))
x[(1L:n) + n * (unclass(class_column) - 1L)] <- 1
dimnames(x) <- list(names(class_column), levels(class_column))
train_scaled <- cbind(train_scaled[,1:8], x)
names(train_scaled) <- c(names(train_scaled)[1:8], "neg", "pos")
library(neuralnet)
set.seed(1234)
nnet1 <- neuralnet(f, data=train_scaled, hidden=c(5,2), threshold=0.1, act.fct = "logistic", linear.output=FALSE, lifesign = "minimal")
plot(nnet1)
nnet.results <- compute(nnet1, test_scaled[,1:8])
pred <- max.col(nnet.results$net.result)
mean(test_scaled$diabetes==pred)
library(mlbench)
data("PimaIndiansDiabetes2")
df <- PimaIndiansDiabetes2[complete.cases(PimaIndiansDiabetes2[]),]
set.seed(1234)
i <- sample(1:nrow(df), 0.75*nrow(df), replace=FALSE)
train <- df[i,]
test <- df[-i,]
glm1 <- glm(diabetes~., data=train, family=binomial)
probs <- predict(glm1, newdata=test, type="response")
pred1 <- ifelse(probs>0.5, "pos", "neg")
mean(pred1==test$diabetes)
scaled <- scale(df[,-9])
df_scaled <- data.frame(cbind(scaled, df$diabetes))
colnames(df_scaled) <- colnames(df)
df_scaled$diabetes <- factor(df_scaled$diabetes)
train_scaled <- df_scaled[i,]
test_scaled <- df_scaled[-i,]
class_column <- train_scaled$diabetes
n <- length(class_column)
x <- matrix(0, n, length(levels(class_column)))
x[(1L:n) + n * (unclass(class_column) - 1L)] <- 1
dimnames(x) <- list(names(class_column), levels(class_column))
train_scaled <- cbind(train_scaled[,1:8], x)
names(train_scaled) <- c(names(train_scaled)[1:8], "neg", "pos")
n <- names(train)
f <- as.formula(paste("neg + pos ~", paste(n[!n %in% "diabetes"], collapse = " + ")))
library(mlbench)
data("PimaIndiansDiabetes2")
df <- PimaIndiansDiabetes2[complete.cases(PimaIndiansDiabetes2[]),]
set.seed(1234)
i <- sample(1:nrow(df), 0.75*nrow(df), replace=FALSE)
train <- df[i,]
test <- df[-i,]
glm1 <- glm(diabetes~., data=train, family=binomial)
probs <- predict(glm1, newdata=test, type="response")
pred1 <- ifelse(probs>0.5, "pos", "neg")
mean(pred1==test$diabetes)
scaled <- scale(df[,-9])
df_scaled <- data.frame(cbind(scaled, df$diabetes))
colnames(df_scaled) <- colnames(df)
df_scaled$diabetes <- factor(df_scaled$diabetes)
train_scaled <- df_scaled[i,]
test_scaled <- df_scaled[-i,]
class_column <- train_scaled$diabetes
n <- length(class_column)
x <- matrix(0, n, length(levels(class_column)))
x[(1L:n) + n * (unclass(class_column) - 1L)] <- 1
dimnames(x) <- list(names(class_column), levels(class_column))
train_scaled <- cbind(train_scaled[,1:8], x)
names(train_scaled) <- c(names(train_scaled)[1:8], "neg", "pos")
n <- names(train)
f <- as.formula(paste("neg + pos ~", paste(n[!n %in% "diabetes"], collapse = " + ")))
library(neuralnet)
set.seed(1234)
nnet1 <- neuralnet(f, data=train_scaled, hidden=c(5,2), threshold=0.1, act.fct = "logistic", linear.output=FALSE, lifesign = "minimal")
plot(nnet1)
nnet.results <- compute(nnet1, test_scaled[,1:8])
pred <- max.col(nnet.results$net.result)
mean(test_scaled$diabetes==pred)
plot(nnet1)
library(MASS)
data("Boston")
df_boston <- Boston[]
set.seed(1234)
i <- sample(1:nrow(Boston), 0.75*nrow(Boston), replace=FALSE)
train <- df_boston[i,]
test <- df_boston[-i,]
lm1 <- lm(medv~., data=train)
summary(lm1)
pred1 <- predict(lm1, newdata=test)
cor1 <- cor(pred1, test$medv)
mse1 <- mean((pred1-test$medv)^2)
# function to normalize the data
normalize <- function(x){
return ((x - min(x)) / (max(x) - min(x)))
}
# apply to all columns
train_scaled <- as.data.frame(lapply(train, normalize))
test_scaled <- as.data.frame(lapply(test, normalize))
library(neuralnet)
n <- names(train)
f <- as.formula(paste("medv ~", paste(n[!n %in% "medv"], collapse = " + ")))
set.seed(1234)
nn1 <- neuralnet(f, data=train_scaled, hidden=c(6, 3),
linear.output = TRUE)
plot(nn1)
pred2 <- compute(nn1, test_scaled[,1:13])
# scaled correlation
cor2_scaled <- cor(pred2$net.result, test_scaled$medv)
# unscaled original values correlation
pred2_unscaled <- pred2$net.result * (max(test$medv) - min(test$medv)) + min(test$medv)
cor2_unscaled <- cor(pred2_unscaled, test_scaled$medv)
mse2 <- mean((pred2_unscaled - test$medv)^2)
library(Keras)
library(keras)
df <- read.csv("PRSA_data.csv", header=TRUE)
df <- df[complete.cases(df), c(3, 6:13)]
head(df)
str(df)
N <- nrow(df)
p <- ncol(df)
t <- 2
X <- df[, -t]
Y <- df[, t]
set.seed(1234)
i <- sample(1:nrow(df), 0.8*nrow(df), replace=FALSE)
set.seed(1234)
i <- sample(1:nrow(df), 0.8*nrow(df), replace=FALSE)
X_train <- data.matrix(X[i, -t])
Y_train <- Y[i]
X_test <- data.matrix(X[-i, -t])
Y_test <- Y[-i]
means <- apply(X_train, 2, mean)
stdvs <- apply(X_train, 2, sd)
X_train <- scale(X_train, center=means, scale=stdvs)
X_test <- scale(X_test, center=means, scale=stdvs)
# build a model
model <- keras_model_sequential()
model %>%
layer_dense(units=64, activation='relu', input_shape = dim(X_train)[[2]]) %>%
layer_dense(units=64, activation='relu') %>%
layer_dense(units=1)
model %>% compile(
loss = 'mse',
optimizer = 'rmsprop',
metrics = c("mae")
)
model %>% fit(X_train, Y_train, epochs=100, batch_size=1, verbose=1)
model %>% fit(X_train, Y_train, epochs=10, batch_size=1, verbose=1)
results <- model %>% evaluate(X_test, Y_test, verbose=0)
results$mean_absolute_error
# how do you get predictions?
pred <- predict(model, X_test)
cor(pred, Y_test)  # 0.948799
mse <- mean((pred - Y_test)^2)
sqrt(mse)
# rmse =3.24, is higher than the absolute mean error
mae <- mean(abs(pred - Y_test))
# build a model
model <- keras_model_sequential()
model %>%
layer_dense(units=16, activation='relu', input_shape = dim(X_train)[[2]]) %>%
layer_dense(units=16, activation='relu') %>%
layer_dense(units=1)
model %>% compile(
loss = 'mse',
optimizer = 'rmsprop',
metrics = c("mae")
)
model %>% fit(X_train, Y_train, epochs=10, batch_size=1, verbose=1)
# build a model
model <- keras_model_sequential()
model %>%
layer_dense(units=16, activation='relu', input_shape = dim(X_train)[[2]]) %>%
#layer_dense(units=16, activation='relu') %>%
layer_dense(units=1)
model %>% compile(
loss = 'mse',
optimizer = 'rmsprop',
metrics = c("mae")
)
model %>% fit(X_train, Y_train, epochs=10, batch_size=1, verbose=1)
# build a model
model <- keras_model_sequential()
model %>%
layer_dense(units=64, activation='relu', input_shape = dim(X_train)[[2]]) %>%
#layer_dense(units=16, activation='relu') %>%
layer_dense(units=1)
model %>% compile(
loss = 'mse',
optimizer = 'rmsprop',
metrics = c("mae")
)
model %>% fit(X_train, Y_train, epochs=10, batch_size=1, verbose=1)
# build a model
model <- keras_model_sequential()
model %>%
layer_dense(units=8, activation='relu', input_shape = dim(X_train)[[2]]) %>%
#layer_dense(units=16, activation='relu') %>%
layer_dense(units=1)
model %>% compile(
loss = 'mse',
optimizer = 'rmsprop',
metrics = c("mae")
)
model %>% fit(X_train, Y_train, epochs=10, batch_size=1, verbose=1)
# build a model
model <- keras_model_sequential()
model %>%
layer_dense(units=16, activation='relu', input_shape = dim(X_train)[[2]]) %>%
layer_dense(units=16, activation='relu') %>%
layer_dense(units=1)
model %>% compile(
loss = 'mse',
optimizer = 'rmsprop',
metrics = c("mae")
)
model %>% fit(X_train, Y_train, epochs=10, batch_size=1, verbose=1)
# try on smaller data
j <- sample(1:nrow(X_train), 10000, replace=FALSE)
model %>% fit(X_train[j,], Y_train[j], epochs=10, batch_size=1, verbose=1)
results <- model %>% evaluate(X_test, Y_test, verbose=0)
results$mean_absolute_error
# how do you get predictions?
pred <- predict(model, X_test)
cor(pred, Y_test)  # 0.476
mse <- mean((pred - Y_test)^2)   # 6596
sqrt(mse)  # 81.2
mae <- mean(abs(pred - Y_test))  # 55.7
str(X)
X_train <- data.matrix(X[i,])
X_test <- data.matrix(X[-i,])
means <- apply(X_train, 2, mean)
stdvs <- apply(X_train, 2, sd)
X_train <- scale(X_train, center=means, scale=stdvs)
X_test <- scale(X_test, center=means, scale=stdvs)
# build a model
model <- keras_model_sequential()
model %>%
layer_dense(units=16, activation='relu', input_shape = dim(X_train)[[2]]) %>%
layer_dense(units=16, activation='relu') %>%
layer_dense(units=1)
model %>% compile(
loss = 'mse',
optimizer = 'rmsprop',
metrics = c("mae")
)
# try on smaller data
j <- sample(1:nrow(X_train), 10000, replace=FALSE)
model %>% fit(X_train[j,], Y_train[j], epochs=10, batch_size=1, verbose=1)
str(X_train)
head(X_train)
# build a model
model <- keras_model_sequential()
model %>%
layer_dense(units=16, activation='relu', input_shape = dim(X_train)[[2]]) %>%
layer_dense(units=16, activation='relu') %>%
layer_dense(units=1)
model %>% compile(
loss = 'mse',
optimizer = 'rmsprop',
metrics = c("mae")
)
# try on smaller data
j <- sample(1:nrow(X_train), 10000, replace=FALSE)
model %>% fit(X_train[j,], Y_train[j], epochs=10, batch_size=1, verbose=1)
results <- model %>% evaluate(X_test, Y_test, verbose=0)
results$mean_absolute_error
# how do you get predictions?
pred <- predict(model, X_test)
cor(pred, Y_test)  # 0.476
mse <- mean((pred - Y_test)^2)   # 6596
sqrt(mse)  # 81.2
model %>% fit(X_train, Y_train, epochs=100, batch_size=1, verbose=1)
model %>% fit(X_train, Y_train, epochs=100, batch_size=100, verbose=1)
results <- model %>% evaluate(X_test, Y_test, verbose=0)
results$mean_absolute_error
# how do you get predictions?
pred <- predict(model, X_test)
cor(pred, Y_test)  # 0.476
mse <- mean((pred - Y_test)^2)   # 6596
sqrt(mse)  # 81.2
mae <- mean(abs(pred - Y_test))  # 55.7
model %>% fit(X_train, Y_train, epochs=100, batch_size=10, verbose=1)
model %>% fit(X_train, Y_train, epochs=100, batch_size=1000, verbose=1)
results <- model %>% evaluate(X_test, Y_test, verbose=0)
results$mean_absolute_error
# how do you get predictions?
pred <- predict(model, X_test)
cor(pred, Y_test)  # 0.7
mse <- mean((pred - Y_test)^2)   # 4269
sqrt(mse)  # 65.3
mae <- mean(abs(pred - Y_test))  # 44.4
# build a model
model <- keras_model_sequential()
model %>%
layer_dense(units=64, activation='relu', input_shape = dim(X_train)[[2]]) %>%
layer_dense(units=64, activation='relu') %>%
layer_dense(units=1)
model %>% compile(
loss = 'mse',
optimizer = 'rmsprop',
metrics = c("mae")
)
model %>% fit(X_train, Y_train, epochs=100, batch_size=1000, verbose=1)
# build a model
model <- keras_model_sequential()
model %>%
layer_dense(units=32, activation='relu', input_shape = dim(X_train)[[2]]) %>%
layer_dense(units=32, activation='relu') %>%
layer_dense(units=1)
model %>% compile(
loss = 'mse',
optimizer = 'rmsprop',
metrics = c("mae")
)
model %>% fit(X_train, Y_train, epochs=100, batch_size=1000, verbose=1)
# build a model
model <- keras_model_sequential()
model %>%
layer_dense(units=16, activation='relu', input_shape = dim(X_train)[[2]]) %>%
layer_dense(units=16, activation='relu') %>%
layer_dense(units=1)
model %>% compile(
loss = 'mse',
optimizer = 'rmsprop',
metrics = c("mae")
)
model %>% fit(X_train, Y_train, epochs=100, batch_size=1000, verbose=1)
model %>% fit(X_train, Y_train, epochs=100, batch_size=100, verbose=1)
# build a model
model <- keras_model_sequential()
model %>%
layer_dense(units=16, activation='relu', input_shape = dim(X_train)[[2]]) %>%
layer_dense(units=16, activation='relu') %>%
layer_dense(units=1)
model %>% compile(
loss = 'mse',
optimizer = 'rmsprop',
metrics = c("mae")
)
model %>% fit(X_train, Y_train, epochs=100, batch_size=100, verbose=1)
results <- model %>% evaluate(X_test, Y_test, verbose=0)
results$mean_absolute_error
# how do you get predictions?
pred <- predict(model, X_test)
cor(pred, Y_test)  # 0.7
mse <- mean((pred - Y_test)^2)   # 4211
sqrt(mse)  # 64.9
mae <- mean(abs(pred - Y_test))  # 44.4
library(MASS)
data("Boston")
df_boston <- Boston[]
set.seed(1234)
i <- sample(1:nrow(Boston), 0.75*nrow(Boston), replace=FALSE)
train <- df_boston[i,]
test <- df_boston[-i,]
lm1 <- lm(medv~., data=train)
summary(lm1)
pred1 <- predict(lm1, newdata=test)
cor1 <- cor(pred1, test$medv)
mse1 <- mean((pred1-test$medv)^2)
# function to normalize the data
normalize <- function(x){
return ((x - min(x)) / (max(x) - min(x)))
}
# apply to all columns
train_scaled <- as.data.frame(lapply(train, normalize))
test_scaled <- as.data.frame(lapply(test, normalize))
library(neuralnet)
n <- names(train)
f <- as.formula(paste("medv ~", paste(n[!n %in% "medv"], collapse = " + ")))
set.seed(1234)
nn1 <- neuralnet(f, data=train_scaled, hidden=c(6, 3),
linear.output = TRUE)
plot(nn1)
pred2 <- compute(nn1, test_scaled[,1:13])
# scaled correlation
cor2_scaled <- cor(pred2$net.result, test_scaled$medv)
# unscaled original values correlation
pred2_unscaled <- pred2$net.result * (max(test$medv) - min(test$medv)) + min(test$medv)
cor2_unscaled <- cor(pred2_unscaled, test_scaled$medv)
mse2 <- mean((pred2_unscaled - test$medv)^2)
pred2 <- compute(nn1, test_scaled[,1:13])
# scaled correlation
cor2_scaled <- cor(pred2$net.result, test_scaled$medv)
# unscaled original values correlation
pred2_unscaled <- pred2$net.result * (max(test$medv) - min(test$medv)) + min(test$medv)
cor2_unscaled <- cor(pred2_unscaled, test_scaled$medv)
mse2 <- mean((pred2_unscaled - test$medv)^2)
nn1 <- neuralnet(f, data=train_scaled, hidden=c(6, 3),
linear.output = TRUE)
pred2 <- compute(nn1, test_scaled[,1:13])
# scaled correlation
cor2_scaled <- cor(pred2$net.result, test_scaled$medv)
# unscaled original values correlation
pred2_unscaled <- pred2$net.result * (max(test$medv) - min(test$medv)) + min(test$medv)
cor2_unscaled <- cor(pred2_unscaled, test_scaled$medv)
mse2 <- mean((pred2_unscaled - test$medv)^2)
set.seed(1234)
nn1 <- neuralnet(f, data=train_scaled, hidden=c(6, 3),
linear.output = TRUE)
library(MASS)
data("Boston")
df_boston <- Boston[]
set.seed(1234)
i <- sample(1:nrow(Boston), 0.75*nrow(Boston), replace=FALSE)
train <- df_boston[i,]
test <- df_boston[-i,]
lm1 <- lm(medv~., data=train)
summary(lm1)
pred1 <- predict(lm1, newdata=test)
cor1 <- cor(pred1, test$medv)
mse1 <- mean((pred1-test$medv)^2)
