---
title: "SVM Regression on Boston Housing Data"
author: "Karen Mazidi"
output:
  pdf_document: default
  html_notebook: default
---

### Load the data

Read more about the data by typing "?Boston" at the console.

```{r}
library(MASS)
df <- Boston[]
str(df)
```
### Train and test

Divide the data into 80% train and 20% test.

```{r}
set.seed(1234)
i <- sample(nrow(Boston), 0.8*nrow(Boston), replace=FALSE)
train <- df[i,]
test <- df[-i,]
```

### Linear regression

Build a linear regression model on the training data.

```{r}
lm1 <- lm(medv~., data=train)
summary(lm1)
```

### Evaluate on the test data

We have 90% correlation between the predicted and target home prices. The mse of 18.9 is a rmse of 4.347, so we are off by about $4,347 on average for the homes in the neighborhood. That's pretty good.

```{r}
pred_lm <- predict(lm1, newdata=test)
cor(pred_lm, test$medv)
mse_lm <- mean((pred_lm - test$medv)^2)
mse_lm
```

### SVM Linear

Now we try SVM regression with a linear kernel. 

```{r}
library(e1071)
svm_fit1 <- svm(medv~., data=train, kernel="linear", cost=10, scale=FALSE)
summary(svm_fit1)
svm_pred1 <- predict(svm_fit1, newdata=test)
cor(svm_pred1, test$medv)
mse_svm1 <- mean((svm_pred1 - test$medv)^2)
mse_svm1
```

### The tune() function

The linear SVM did slightly better than linear regression. Next we perform some tuning to find the best cost parameter. The tune() function tries to find optimal hyperparameters for the svm using a grid search. This involves trying all the suggested parameters in a cross-validation scheme. Here we asked it to try several different cost parameters. The best model can be extracted from the tune results. 


```{r}
tune_svm1 <- tune(svm, medv~., data=train, kernel="linear",
               ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune_svm1)
best_mod1 <- tune_svm1$best.model
summary(best_mod1)
```
### Use the best model

The best model parameters were selected on the train set. It will not necessarily perform better on the test data, and indeed it performed slightly worse.

```{r}
svm_pred2 <- predict(best_mod1, newdata=test)
cor(svm_pred2, test$medv)
mse_svm2 <- mean((svm_pred2 - test$medv)^2)
mse_svm2
```
### Try the radial kernel

For radial kernel we have an additional hyperparameter, gamma.

```{r}
svm_fit2 <- svm(medv~., data=train, kernel="radial", cost=1, gamma=1, scale=FALSE)
summary(svm_fit2)
svm_pred2 <- predict(svm_fit2, newdata=test)
cor(svm_pred2, test$medv)
mse_svm2 <- mean((svm_pred2 - test$medv)^2)
mse_svm2
```
### Tune the hyperparameters

```{r}
set.seed(1234)
tune_svm2 = tune(svm, medv~., data=train, kernel="radial",
                ranges=list(cost=c(0.1,1,10,100,1000),
                gamma=c(0.5,1,2,3,4)))
summary(tune_svm2)

```
### Again, best model isn't. 

```{r}
svm_pred3 <- predict(tune_svm2$best.model, newdata=test)
cor(svm_pred3, test$medv)
mse_svm3 <- mean((svm_pred3 - test$medv)^2)
mse_svm3
```

