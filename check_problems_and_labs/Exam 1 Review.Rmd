---
title: "R Notebook"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

This notebook runs through some examples to emphasize important material for Exam 1. The notebook uses a wine data set that was edited from the wine data sets on the UCI repository.

Note: You won't have to write R code on the exam but you need to understand code and its output.

### Load the Data

First we read in the data and take a look.

```{r}
wine <- read.csv("wine_all.csv", header=TRUE)
str(wine)
```

* Which columns are quantitative?
* Which columns are qualitative?
* What is a factor?

### Divide data into train and test

* Why do we set a seed?
* What does sample do?
* How did we subset the data?
* Why do we divide into train and test sets?


```{r}
set.seed(1234)
i <- sample(1:nrow(wine), 0.8*nrow(wine), replace=FALSE)
train <- wine[i,]
test <- wine[-i,]
```

## Build a linear regression model

We will try to predict quality from all other factors.

* Explain the lm() function
* Explain the output of summary including:
  * What is the formula?
  * Is the intercept considered a "predictor"? Why or why not?
  * What are residuals?
  * Which predictors seem good, and why?
  * What is RSE?
  * What is R-squared?
  * What is the F-statistic?
* Are these metrics for the train or test set?
* What is a dummy variable? Do you see one below?
  

```{r}
lm1 <- lm(quality~., data=train)
summary(lm1)
```
### Plot the Residuals

* What do we hope to see?
* What are outliers?
* What are leverage points?

```{r}
par(mfrow=c(2,2))
plot(lm1)
```


### Evaluate on the Test Data

* What does pred contain?
* What is correlation? Did you get a good correlation?
* What is mse?
* How do we compute mse for the test set? For the train set?
* Why might rmse be easier to interpret?
* Look at some predictions vs. actual values.

```{r}
pred <- predict(lm1, newdata=test)
cor_lm <- cor(pred, test$quality)
mse_lm <- mean((pred-test$quality)^2)
mse_train <- mean(lm1$residuals^2)
rmse_lm <- sqrt(mse_lm)
print(cbind(head(pred, n=10), head(test$quality, n=10)))
```
### Build another Linear Regression model

* What is anova?
* How do we interpret the results?
* If a linear regression model has predictors with low p-values, should we take them out? Why or why not?

```{r}
lm2 <- lm(quality~volatile_acidity+residual_sugar+alcohol+sulphates+type+type*alcohol, data=train)
anova(lm1, lm2)
```



### Linear Regression

* supervised or unsupervised?
* classification or regression?
* parametric or non-parametric?
* bias (strong assumptions about the shape of the data) and variance?
* what are some assumptions of the linear model?
* what is a confounding variable? How can we detect them?
* how do we model interaction effects?
* can a linear model tell us about correlation or causation or both between x and y?
* does a linear model have to be a straight line? Why or why not?


## Build a Logistic Regression Model

We want to predict red/white given all other predictors.

* Why do we need "family=binomial"
* What is a glm?
* Can you interpret the coefficients in logistic regression the same as we did for linear regression?
* Explain null deviance v. residual deviance. 
* What does AIC tell us?

```{r}
glm1 <- glm(type~., data=train, family=binomial)
summary(glm1)
```

### Check the Train and Test

* What is this code doing, and why?

```{r}
summary(train$type)
summary(test$type)
```

### Evaluate on the Test Data

* What does probs look like?
* What would it look like without type="response"?
* How do we translate probabilities into predictions?
* How do we know what integers 'type' was coded in?
* What does the table show us?
* Why do we use accuracy as a metric instead of cor or mse?

```{r}
probs <- predict(glm1, newdata=test, type="response")
pred_glm <- ifelse(probs>0.5, 2, 1)
table(pred_glm, test$type)
acc_glm <- mean(pred_glm==as.integer(test$type))
```

### Logistic Regression

* supervised or unsupervised?
* classification or regression?
* parametric or non-parametric?
* bias and variance?
* are the coefficients computed directly or by optimization methods?

## kNN

First try kNN classification with unscaled data. 

* How did this compare to logistic regression?
* What does k mean in kNN?

### kNN Classification

```{r}
library(class)
knn_pred <- knn(train=train[,1:12], test=test[,1:12], cl=train$type, k=3)
table(knn_pred, test$type)
acc_knn <- mean(knn_pred==test$type)
```

Now scale the data and try again.

* Compare to the previous results.
* Should we try different values of k? Why or why not?


```{r}
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}
train_norm <- as.data.frame(lapply(train[,1:12], normalize))
test_norm <- as.data.frame(lapply(test[,1:12], normalize))
knn_pred2 <- knn(train=train_norm, test=test_norm, cl=train$type, k=3)
table(knn_pred2, test$type)
acc_knn_scaled <- mean(knn_pred2==test$type)
```


### kNN Regression

* Why did we leave out type?


```{r}
train_reg <- train_norm[,1:11]
test_reg <- test_norm[,1:11]
library(caret)
fit <- knnreg(train_reg, train$quality, k=3)
predictions <- predict(fit, test_reg)
cor_knn <- cor(predictions, test$quality) 
mse_knn <- mean((predictions - test$quality)^2) 
```

### try different values of k

* What is the best k?
* How does it compare to the other models?

```{r}
test_mse_knn <- rep(0, 40)
test_cor_knn <- rep(0, 40)
for (i in 1:40){
  fit <- knnreg(train_reg, train$quality, k=i)
  pred <- predict(fit, newdata=test_reg)
  test_cor_knn[i] <- cor(pred, test$quality)
  test_mse_knn[i] <- mean((pred - test$quality)^2)
}
which.min(test_mse_knn)
which.max(test_cor_knn)

test_mse_knn[22]  # .547
test_cor_knn[22]  # .548
```


### kNN

* supervised or unsupervised?
* classification or regression?
* parametric or non-parametric?
* bias and variance?
* advantages?
* disadvantages?

## k-means

```{r}
set.seed(1234)
df <- wine[]
df <- as.data.frame(scale(df[,-13]))  # remove type
fit.km <- kmeans(df, 2, nstart=20)
fit.km
```

### Interpreting clusters

The within-ss for the two clusters are large. This is probably due to the large number of predictors. 

Looking at the correlation of the clusters and the type red/white, we seem to have found something in the data. 
```{r}
cor(fit.km$cluster, abs(1-as.integer(wine$type)))  # flip 1 and 2
```

### Plot

```{r}
# plot is too dense with all data so randomly select 250
j <- sample(1:nrow(wine), 250, replace=FALSE)
# plot - using pH and alcohold to spread out the points
# color = white/red wine
# shape = cluster identified by kmeans
plot(wine$pH[j], wine$alcohol[j], pch=fit.km$cluster[j], 
     col=c("brown3","goldenrod1")[as.integer(wine$type[j])],
     xlab="alcohol", ylab="pH")
# only plotting in 2 dimensions, in 13-d space, white and red wines must be "close"
```

### Try Various K

```{r}
wsplot <- function(data, nc=15, seed=1234){ 
  wss <- (nrow(data)-1)*sum(apply(data,2,var)) 
  for (i in 2:nc){
    set.seed(seed)
    wss[i] <- sum(kmeans(data,centers=i)$withinss) 
  }
  plot(1:nc, wss, type="b", xlab="Number of Clusters", ylab="Within groups sum of squares")
} 
wsplot(df)  # elbow at 3
```

```{r}
fit.km3 <- kmeans(df, 3, nstart=20)
fit.km3
fit.km$withinss
fit.km3$withinss  # lower
```

### Hierarchical clustering

```{r}
d <- dist(wine)  # data was normalized above
fit.average <- hclust(d, method="averag")
plot(fit.average, hang=-1, cex=.8)
```

Try fewer examples so we can see the data.

```{r}
j <- sample(1:nrow(wine), 10, replace=FALSE)
d <- dist(wine[j,])  # data was normalized above
fit.small <- hclust(d, method="averag")
plot(fit.small, hang=-1, cex=.8)
```
Compare some values.

```{r}
# compare 5698 and 4705 == -1.9
sum(df[5698,] - df[4705,])
# compare 3555 and 6127 == 6.84
sum(df[3555,] - df[6127,])  # diff = -1.07, 3 times as much
```




## General ML Questions

* What is an indication that you have overfit the data?
* What is an indication that you have underfit the data?
* What is the relation between underfitting/underfitting and bias/variance?

## Other Things to Study

* Loss and cost functions for linear regression, logistic regression
* How these functions are used for linear regression, logistic regression
* Matrix notation for data
* gradient descent
* k-fold cross validation
* parameter versus hyper-parameter
* a couple of questions about k-means, hierarchical clustering





