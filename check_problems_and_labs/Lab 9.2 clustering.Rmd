---
title: "Clustering"
author: Karen Mazidi
output: html_notebook
---

Using the Glass data set in package mlbench, we will try different clustering algorithms. 
The data has 214 observations with 9 variables indicating chemical composition of a glass sample, and one factor column indicating one of 7 different glass types. This data set was inspired by crime scene analysis.


### Load the data

```{r}
library(mlbench)
data(Glass)
df <- Glass[]
str(df)
summary(df$Type)
```

# knn

Try the knn algorithm to see if it can classify glass types based on chemical composition. We will try k=7 neighbors to start with.

Accuracy on the unscaled data was 56% and on the unscaled data it was 48%.

```{r}
library(class)
set.seed(1234)
i <- sample(1:nrow(df), 0.8*nrow(df), replace=FALSE)
train <- df[i,]
test <- df[-i,]

glass_pred <- knn(train=train[,-10], test=test[,-10], cl=train[,10], k=7)
acc <- mean(glass_pred == test$Type)
table_knn <- table(glass_pred, test$Type)
table_knn

# see if scaling helps
train_scale <- as.data.frame(lapply(train[,1:9], scale))
test_scale <- as.data.frame(lapply(test[,1:9], scale))
glass_pred <- knn(train=train_scale, test=test_scale, cl=train$Type, k=7)
acc_scaled <- mean(glass_pred == test$Type)
table(glass_pred, test$Type)
# worse performance when scaling
```
### Find best k for kNN

Trying k values from 1 to 10, k=1 seems best, with accuracy at nearly 70%.

```{r}
acc_k <- rep (0, 10)
i <- 1
for (k in 1:10){
  fit_k <- knn(train=train[,-10], test=test[,-10], cl=train[,10], k=k)
  acc_k[i] <- mean(fit_k == test$Type)
  print(paste("k=", k, acc_k[i]))
  i <- i + 1
}
# k=1 is best
```


### k-means

What can we learn about the data from k-means?

Let's try k=7 clusters to see what happens.

Looking at the table below does not seem promising. We will look at some metrics later in the notebook.

```{r}
set.seed(1234)
glass_cluster <- kmeans(df[,-10], 7, nstart=20)
glass_cluster
table_7 <- table(df$Type, glass_cluster$cluster)
table_7
```

Check the glass types in each cluster. Some clusters look fairly homogenous in terms of glass type. Others, not at all.

```{r}
for (i in 1:7){
  print(paste("Cluster ", i))
  ind <- which(glass_cluster$cluster==i)
  print(df[ind,10])
}
```

### Try different values for k

Try from 2 to 12 clusters. There seems to be an elbow at k=4.

```{r}
plot_withinss <- function(df, max_clusters){
  withinss <- rep(0, max_clusters-1)
  for (i in 2:max_clusters){
    set.seed(1234)
    withinss[i] <- sum(kmeans(df, i)$withinss)
  }
  plot(2:max_clusters, withinss[2:max_clusters], type="o", 
      xlab="K", ylab="Within Sum Squares")
}
plot_withinss(df, 12)
# there seems to be an elbow at 4
```

### Try k=4

```{r}
set.seed(1234)
glass_cluster <- kmeans(df[,-10], 4, nstart=20)
glass_cluster
table_4 <- table(df$Type, glass_cluster$cluster)
table_4
for (i in 1:4){
  print(paste("Cluster ", i))
  ind <- which(glass_cluster$cluster==i)
  print(df[ind,10])
}
```
### Rand Index

There are many metrics for cluster analysis. Here we just look at one, the Rand Index which ranges from -1 (no agreement) to +1 (perfect agreement).

The Rand index indicates that k=7 was better.

```{r}
library(flexclust)
randIndex(table_4)  # 0.25
randIndex(table_7)  # 0.27
```


### NbClust

Let's get the opinion of NbClust on the best k. The k=3 looks best.

```{r}
library(NbClust)
set.seed(1234)
nc <- NbClust(df[,-10], min.nc=2, max.nc=15, method="kmeans")
table(nc$Best.n[1])
barplot(table(nc$Best.n[1,]))
```

# try k=3

The Rand Index did not agree that k=3 was best. 

```{r}
set.seed(1234)
glass_cluster <- kmeans(df[,-10], 3, nstart=20)
glass_cluster
for (i in 1:3){
  print(paste("Cluster ", i))
  ind <- which(glass_cluster$cluster==i)
  print(df[ind,10])
}
table_3 <- table(df$Type, glass_cluster$cluster)
table_3
randIndex(table_3)
```

### Hierarchical

Let's try hierarchical clustering with this data. First we get the distance measures then display the dendogram. It looks like 4 or 5 might be good cut off points. 

```{r}
d <- dist(df)
fit.average <- hclust(d, method="average")
plot(fit.average)
```

### cut the cluster at 3, 4, 5 and 6

A cut at 6 looks best but 4, 5 and 6 are similar.

```{r}
for (c in 3:6){
  cluster_cut <- cutree(fit.average, c)
  table_cut <- table(cluster_cut, df$Type)
  print(table_cut)
  ri <- randIndex(table_cut)
  print(paste("cut=", c, "Rand index = ", ri))
}

```

